# Project--Hadoop-10


<table>
  
**In this project we will integrate Apache Spark with MongoDB and it's gonna be amazing combo you goona have in your toolkit after this project**.<br></br>

Lets start by learning about Spark and MongoDB<br></br>

**What is Spark?** <br></br>

Apache Spark is a multi-language engine for executing data engineering, data science, and machine learning on single-node machines or clusters.<br></br>

**What is MongoDB?** <br></br>

MongoDB is a source-available cross-platform document-oriented database program. Classified as a NoSQL database program, MongoDB uses JSON-like documents with optional schemas.<br></br>

The MongoDB Connector for Apache Spark exposes all of Spark's libraries, including Scala, Java, Python and R. MongoDB data is materialized as DataFrames and Datasets for analysis with machine learning, graph, streaming, and SQL APIs.<br></br>







After this all worked fine jump to the python code file on getting to know about how to do integration from spark to cassandra.<br></br>

**Main Insights of Project**:<br></br>

First we made the dataframe from the u.user file which is uploaded alongside the code file. Go through the dataset before jumping to the code. Then we writed the dataframe 
with all the configuration needed into Cassandra. We can check whether it is writed in Cassandra or not using 'SELECT * from users;' command.<br></br>

Secondly we read the Dataframe from cassandra and then we create views and even run sql queries on that.<br></br>

</table>


**So what you are you waiting for..? Jump to the code to get started. As usual for any doubt or query see you in pull request section üòÅüòÇ. Thanks!**





