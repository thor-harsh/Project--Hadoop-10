# Project--Hadoop-10


<table>
  
**In this project we will integrate Apache Spark with MongoDB and it's gonna be amazing combo you goona have in your toolkit after this project**.<br></br>

Lets start by learning about Spark and MongoDB: <br></br>

**What is Spark?** <br></br>

Apache Spark is a multi-language engine for executing data engineering, data science, and machine learning on single-node machines or clusters.<br></br>

**What is MongoDB?** <br></br>

MongoDB is a source-available cross-platform document-oriented database program. Classified as a NoSQL database program, MongoDB uses JSON-like documents with optional schemas.<br></br>

The MongoDB Connector for Apache Spark exposes all of Spark's libraries, including Scala, Java, Python and R. MongoDB data is materialized as DataFrames and Datasets for analysis with machine learning, graph, streaming, and SQL APIs.<br></br>


So lets jump to the python code file on getting to know about how to do integration from spark to cassandra.<br></br>

**Main Insights of Project**:<br></br>

First we made the dataframe from the u.user file which is uploaded alongside the code file. Go through the dataset before jumping to the code. Then we writed the dataframe 
with all the configuration needed into MongoDB. We can check whether it is writed in MongoDB or not using 'SELECT * from users;' command.<br></br>

Secondly we read the Dataframe from cassandra and then we create views and even run sql queries on that.<br></br>

**Important Note:** You can do amazing stuff with MongoDB irrespective of any other NoSQL Database as it has its own file system which is called GFL same as hadoop has which is called HDFS and you can literlly do all the things on it which you can do in hadoop.<br></br>

**MongoDB vs Hadoop**<br></br>

Both Hadoop and MongoDB have many benefits over traditional databases when it comes to handling big data. However, only MongoDB can act as a complete replacement for a traditional database. With its flexible schema, MongoDB makes it easy to store information in a format that doesn‚Äôt require many transformations ahead of time. Its query language makes it possible to efficiently access and even process data on the fly.<br></br>

There are still times when you might want to use Hadoop, though. With its distributed file system, Hadoop can come in handy when dealing with massive objects. In these cases, it is possible to use Hadoop to complement MongoDB to leverage the power of both into a single cohesive architecture.

</table>


**So what you are you waiting for..? Jump to the code to get started. As usual for any doubt or query see you in pull request section üòÅüòÇ. Thanks!**





